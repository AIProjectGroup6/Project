# -*- coding: utf-8 -*-
"""AI_Driven_Waste _Management_VGG16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T04Gr9RyYGU6IWzCXSdqfLgHhAyfKv0O

CHANGE LOG

Developer                                 -----------------                     Date Modified      -------------------    Comments

Anand Chathananickal Sajeevan             -----------------                     July 18 2024       -------------------    Initial Version - Data Loading and Data Exploration

Jobina Joy                                -----------------                     July 18 2024       -------------------    Feature Engineering and Data Preprocessing

Jobina Joy                                -----------------                     July 24 2024       -------------------    Added EDA
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive"

import matplotlib.pyplot as plt  # Library for plotting and visualizing data
import numpy as np  # Fundamental package for numerical computations and handling arrays
import os  # Module for interacting with the operating system
import tensorflow as tf  # TensorFlow, a comprehensive open-source platform for machine learning
import tensorflow.keras.layers as tfl  # Keras API for building and training neural networks
from tensorflow.keras.layers import Layer

# Functions for loading and augmenting image datasets
from tensorflow.keras.preprocessing import image_dataset_from_directory  # Load images from a directory and create a dataset
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomContrast  # Layers for random image augmentations
from tensorflow.keras.regularizers import l2  # Apply L2 regularization to layers

# PIL (Python Imaging Library) for image processing
from PIL import ImageEnhance  # Module to enhance images (adjust brightness, contrast, sharpness, color)
from PIL import Image  # Module for opening, manipulating, and saving image files
import cv2
import seaborn as sns

"""

Loading images from a specified directory.

Automatically inferring labels from subdirectory names.

Shuffling and batching the images.

Resizing images to a consistent size.

Splitting the data into training and validation sets.

Verifying the dataset structure by printing out class names and inspecting a batch of images and labels."""

BATCH_SIZE = 32
IMG_SIZE = (224, 224)
directory = "/content/drive/My Drive/images/images"

# Install OpenCV if not already installed
# !pip install opencv-python

import os
import matplotlib.pyplot as plt
import numpy as np
# Helper function to load images
def load_images(directory, max_images=None):
    image_paths = []
    class_names = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(('jpg', 'jpeg', 'png')):
                image_paths.append(os.path.join(root, file))
                class_names.append(os.path.basename(root))
                if max_images and len(image_paths) >= max_images:
                    break
    return image_paths, class_names

# Load images
image_paths, class_names = load_images(directory, max_images=1000)  # Adjust max_images as needed
unique_classes = list(set(class_names))

# EDA: Visualize a few images from the dataset
def show_images(image_paths, num_images=30):
    plt.figure(figsize=(10, 10))
    for i in range(num_images):
        img = Image.open(image_paths[i])
        ax = plt.subplot(5, 6, i + 1)
        plt.imshow(img)
        plt.axis('off')
    plt.show()

show_images(image_paths)

# EDA: Analyze brightness and contrast distributions
def analyze_brightness_contrast(image_paths):
    brightness = []
    contrast = []
    for path in image_paths:
        img = Image.open(path).convert('L')  # Convert image to grayscale
        img = np.array(img)
        brightness.append(np.mean(img))
        contrast.append(np.std(img))
    return brightness, contrast

brightness, contrast = analyze_brightness_contrast(image_paths)

# Plot brightness and contrast distributions
plt.figure(figsize=(12, 6))
plt.hist(brightness, bins=50, alpha=0.7, label='Brightness', color='gold', edgecolor='black')
plt.xlabel('Brightness')
plt.ylabel('Frequency')
plt.title('Distribution of Brightness')
plt.grid(axis='y', linestyle='--', linewidth=0.7)
plt.show()

plt.figure(figsize=(12, 6))
plt.hist(contrast, bins=50, alpha=0.7, label='Contrast', color='purple', edgecolor='black')
plt.xlabel('Contrast')
plt.ylabel('Frequency')
plt.title('Distribution of Contrast')
plt.grid(axis='y', linestyle='--', linewidth=0.7)
plt.show()

# EDA: Analyze color distribution
def analyze_color_distribution(image_paths):
    color_means = []
    color_stds = []
    for path in image_paths:
        img = Image.open(path)
        img = np.array(img)
        color_means.append(np.mean(img, axis=(0, 1)))
        color_stds.append(np.std(img, axis=(0, 1)))
    return color_means, color_stds

color_means, color_stds = analyze_color_distribution(image_paths)
color_means = np.array(color_means)
color_stds = np.array(color_stds)

# EDA: Identify and visualize noisy images
def analyze_noise(image_paths):
    noise_levels = []
    for path in image_paths:
        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
        noise = cv2.Laplacian(img, cv2.CV_64F).var()
        noise_levels.append(noise)
    return noise_levels

noise_levels = analyze_noise(image_paths)

# Plot noise levels distribution
plt.figure(figsize=(12, 6))
plt.hist(noise_levels, bins=50, alpha=0.7, label='Noise Level', color='grey', edgecolor='black')
plt.xlabel('Noise Level')
plt.ylabel('Frequency')
plt.title('Distribution of Noise Levels')
plt.grid(axis='y', linestyle='--', linewidth=0.7)
plt.show()

# EDA: Analyze saturation and hue distributions
def analyze_saturation_hue(image_paths):
    saturation = []
    hue = []
    for path in image_paths:
        img = cv2.imread(path)
        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        hue.append(np.mean(img_hsv[:, :, 0]))
        saturation.append(np.mean(img_hsv[:, :, 1]))
    return saturation, hue

saturation, hue = analyze_saturation_hue(image_paths)

# Plot density plots for saturation and hue
plt.figure(figsize=(12, 6))
sns.kdeplot(saturation, shade=True, color='cyan', label='Saturation')
plt.xlabel('Saturation')
plt.ylabel('Density')
plt.title('Density Plot of Saturation')
plt.legend()
plt.grid(axis='y', linestyle='--', linewidth=0.7)
plt.show()

plt.figure(figsize=(12, 6))
sns.kdeplot(hue, shade=True, color='orange', label='Hue')
plt.xlabel('Hue')
plt.ylabel('Density')
plt.title('Density Plot of Hue')
plt.legend()
plt.grid(axis='y', linestyle='--', linewidth=0.7)
plt.show()

# Define the training dataset
train_dataset = image_dataset_from_directory(
    directory,
    labels='inferred',  # Automatically infers labels from subdirectory names
    label_mode='categorical',  # Return labels as one-hot encoded vectors
    shuffle=True,  # Shuffle the dataset to ensure random distribution
    batch_size=BATCH_SIZE,  # Number of images to process in a single batch
    image_size=IMG_SIZE,  # Resize all images to 224x224 pixels
    validation_split=0.2,  # Reserve 20% of the data for validation
    subset='training',  # This dataset is for training
    seed=42  # Random seed for reproducibility
)

# Define the validation dataset
validation_dataset = image_dataset_from_directory(
    directory,
    labels='inferred',  # Automatically infers labels from subdirectory names
    label_mode='categorical',  # Return labels as one-hot encoded vectors
    shuffle=True,  # Shuffle the dataset to ensure random distribution
    batch_size=BATCH_SIZE,  # Number of images to process in a single batch
    image_size=IMG_SIZE,  # Resize all images to 224x224 pixels
    validation_split=0.2,  # Reserve 20% of the data for validation
    subset='validation',  # This dataset is for validation
    seed=42  # Random seed for reproducibility
)

# Verify the dataset structure
class_names = train_dataset.class_names  # Retrieve the class names inferred from the subdirectory names

# Print each waste class name on a new line
num_classes = len(class_names)
print(f"Number of waste classes: {num_classes}")
print("Waste class names:")
for class_name in class_names:
    print(class_name)

"""Exploring the image quality -- checking the low, high and average image quality

Visualizing the class distribution in both traning and validation dataset.

Image Augmentation -- Image augmentation is performed to enhance the training dataset for machine learning models, particularly in the context of computer vision tasks such as image classification, object detection, and segmentation.

RandomFlip -- Helps the model learn invariant features, improving its ability to recognize objects regardless of their orientation.

RandomRotation -- Enhances the model's robustness to rotation variations, making it more effective at recognizing objects in different orientations.

RandomZoom -- Helps the model handle scale variations, which is important for detecting objects of varying sizes in the input images.

RandomContrast -- Improves the model's ability to handle different lighting conditions and contrasts, making it more robust to changes in illumination.

RandomBrightness -- Enhances the model's robustness to lighting variations, ensuring it can accurately recognize objects in different lighting conditions.

RandomSharpness -- Sharpness adjustments can help the model learn to detect edges and fine details more effectively, which is crucial for tasks requiring high precision, such as medical imaging.

RandomNoise -- Noise addition helps the model become more robust to imperfections and noise in real-world data, which can improve generalization.
"""

plt.figure(figsize=(10, 10))  # Set the figure size for the plot

# Iterate over the first 2 batches from the training dataset
for images, labels in train_dataset.take(2):
    # Plot 12 images from the current batch
    for i in range(12):
        ax = plt.subplot(4, 3, i + 1)  # Create a subplot in a 4x3 grid
        plt.imshow(images[i].numpy().astype("uint8"))  # Display the image
        label_index = tf.argmax(labels[i]).numpy()  # Get the index of the highest value (class)
        plt.title(class_names[label_index])  # Set the title to the class name
        plt.axis("off")  # Hide the axis

plt.show()  # Display the plot

# Check class distribution in the training dataset
class_counts = {class_name: 0 for class_name in class_names}  # Initialize a dictionary to store counts for each class
for _, labels in train_dataset:  # Iterate through batches in the training dataset
    for label in labels:  # Iterate through labels in each batch
        class_index = tf.argmax(label).numpy()  # Get the index of the highest value (class)
        class_name = class_names[class_index]  # Get the class name corresponding to the index
        class_counts[class_name] += 1  # Increment the count for the class

# Plot class distribution for the training dataset
plt.figure(figsize=(10, 6))  # Set the figure size for the plot
plt.bar(class_counts.keys(), class_counts.values())  # Create a bar plot of class counts
plt.xlabel('Class Names')  # Set the label for the x-axis
plt.ylabel('Number of Images')  # Set the label for the y-axis
plt.title('Class Distribution in Training Dataset')  # Set the title of the plot
plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability
plt.show()  # Display the plot

# Check class distribution in the validation dataset
class_counts = {class_name: 0 for class_name in class_names}  # Reinitialize the dictionary for validation dataset
for _, labels in validation_dataset:  # Iterate through batches in the validation dataset
    for label in labels:  # Iterate through labels in each batch
        class_index = tf.argmax(label).numpy()  # Get the index of the highest value (class)
        class_name = class_names[class_index]  # Get the class name corresponding to the index
        class_counts[class_name] += 1  # Increment the count for the class

# Plot class distribution for the validation dataset
plt.figure(figsize=(10, 6))  # Set the figure size for the plot
plt.bar(class_counts.keys(), class_counts.values())  # Create a bar plot of class counts
plt.xlabel('Class Names')  # Set the label for the x-axis
plt.ylabel('Number of Images')  # Set the label for the y-axis
plt.title('Class Distribution in Validation Dataset')  # Set the title of the plot
plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability
plt.show()  # Display the plot

# Function to assess image quality for a single image
    def assess_image_quality(image):
        # Convert PIL Image to numpy array and then to TensorFlow tensor
        image = tf.keras.preprocessing.image.img_to_array(image)
        image = tf.expand_dims(image, axis=0)  # Add batch dimension

        # Ensure image is in RGB format if required
        if image.shape[-1] != 3:
            image = tf.image.grayscale_to_rgb(image)

        # Apply Sobel edges operation
        edge_image = tf.image.sobel_edges(image)

        # Calculate edge magnitude
        edge_magnitude = tf.sqrt(tf.reduce_sum(tf.square(edge_image), axis=-1))

        # Calculate quality score as the mean edge magnitude
        quality_score = tf.reduce_mean(edge_magnitude).numpy()

        return quality_score

    # Function to analyze image quality for a dataset
    def image_quality_analysis(dataset):
        quality_scores = []
        for image_batch, _ in dataset:  # Iterate directly over the dataset
            for image in image_batch:
                quality_score = assess_image_quality(image)
                quality_scores.append(quality_score)
        return quality_scores

    # Example usage for a single image
    image_path = "/content/drive/My Drive/images/images/aluminum_soda_cans/real_world/Image_194.png"
    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))  # Load and resize image
    quality_score = assess_image_quality(image)  # Assess quality of a single image
    print(f"Single Image Quality Score: {quality_score}")

    # Example usage of image_quality_analysis function on a dataset
    quality_scores = image_quality_analysis(train_dataset)  # Assess quality of images in the training dataset
    print(f"Image Quality Scores: {quality_scores}")

# Calculate average image quality score
average_quality = np.mean(quality_scores)

# Calculate mean quality score
mean_quality = np.median(quality_scores)

# Calculate low quality score (minimum)
low_quality = np.min(quality_scores)

# Calculate high quality score (maximum)
high_quality = np.max(quality_scores)

print(f"Average Image Quality Score: {average_quality}")
print(f"Mean Quality Score: {mean_quality}")
print(f"Low Quality Score: {low_quality}")
print(f"High Quality Score: {high_quality}")

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)

class RandomNoise(Layer):
    def __init__(self, mean=0.0, stddev=0.1, **kwargs):
        super().__init__(**kwargs)
        self.mean = mean
        self.stddev = stddev

    def call(self, inputs, training=True):
        if training:
            noise = tf.random.normal(shape=tf.shape(inputs), mean=self.mean, stddev=self.stddev)
            return inputs + noise
        return inputs

class RandomBrightness(Layer):
    def __init__(self, max_delta=0.2, **kwargs):
        super().__init__(**kwargs)
        self.max_delta = max_delta

    def call(self, inputs, training=True):
        if training:
            return tf.image.random_brightness(inputs, max_delta=self.max_delta)
        return inputs

class RandomSharpness(Layer):
    def __init__(self, alpha=1.5, **kwargs):
        super().__init__(**kwargs)
        self.alpha = alpha

    def call(self, inputs, training=True):
        if training:
            # Apply contrast as a proxy for sharpness adjustment
            inputs = tf.image.random_contrast(inputs, lower=0.5, upper=1.5)
        return inputs

class RandomColorJitter(Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs, training=True):
        if training:
            inputs = tf.image.random_hue(inputs, 0.1)  # Adjusted hue range based on histogram Adjust the hue to reflect a range around the mode of the distribution, approximately ±0.1
            inputs = tf.image.random_saturation(inputs, 0.2, 1.2)  # Adjusted saturation range based on histogram Adjust the saturation range to reflect the distribution in the histogram, around 0.2 to 1.2.The saturation histogram shows a right-skewed distribution with most values concentrated between 0 and 60
            inputs = tf.image.random_brightness(inputs, 0.05)
            inputs = tf.image.random_contrast(inputs, 0.7, 1.3)
        return inputs

class RandomContrast(Layer):
    def __init__(self, contrast_factor=0.2, **kwargs):
        super().__init__(**kwargs)
        self.contrast_factor = contrast_factor

    def call(self, inputs, training=True):
        if training:
            return tf.image.random_contrast(inputs, 1 - self.contrast_factor, 1 + self.contrast_factor)
        return inputs

def data_augmenter():
    data_augmentation = tf.keras.Sequential()
    data_augmentation.add(tf.keras.layers.RandomFlip("horizontal_and_vertical"))
    data_augmentation.add(tf.keras.layers.RandomRotation(0.2))
    data_augmentation.add(tf.keras.layers.RandomZoom(0.2))
    data_augmentation.add(RandomContrast(0.2)) # The contrast histogram ranges from 0 to 120, with a significant concentration between 20 and 100. A contrast factor of 0.3 allows the contrast to vary by ±30%. This ensures that the contrast adjustments will not be too extreme, maintaining the visual quality of the images while introducing enough variability to enhance model robustness. For example, an image with a contrast value of 80 can be adjusted to range between 56 (70% of 80) and 104 (130% of 80).
    data_augmentation.add(RandomBrightness(0.2))#The brightness histogram shows a wide range from 0 to 250, with most values between 100 and 250.A factor of 0.3 allows the brightness to vary by ±30% of its original value. This means that an image with a mean brightness value of 200 could be augmented to have a brightness between 140 (70% of 200) and 260 (130% of 200). This range covers most of the natural variability seen in the dataset without making the images too dark or too bright.
    data_augmentation.add(RandomSharpness(alpha=1.5))  # Adjust sharpness using contrast
    data_augmentation.add(RandomNoise(mean=0.0, stddev=20.0)) # Given the central tendency around 1000-2000, we use a scaling factor to set stddev to 0.01 * 2000 = 20. We keep mean at 0.0 since the noise is centered around 0. This ensures that the noise added during augmentation is representative of the dataset.

    return data_augmentation



# Create the data augmentation model
data_augmentation = data_augmenter()

# Visualize the augmented images
for image, _ in train_dataset.take(4):  # Take the first 3 batches from the training dataset
    plt.figure(figsize=(10, 10))  # Set the figure size
    first_image = image[0]  # Get the first image from the batch
    for i in range(12):  # Generate 12 augmented images
        ax = plt.subplot(5, 3, i + 1)  # Create a subplot in a 5x3 grid
        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))  # Apply augmentation
        plt.imshow(augmented_image[0] / 255)  # Display the augmented image
        plt.axis('off')  # Hide the axis
    plt.show()  # Display the plot

# Define a function to apply augmentation to the dataset
def augment(image, label):
    image = data_augmentation(image)  # Apply data augmentation to the image
    return image, label  # Return the augmented image and the label

# Apply the augmentation function to the training dataset
augmented_train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)






"""MODEL BUILDING : VGG16 using transfer learnig"""




from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.metrics import Precision, Recall
from sklearn.metrics import confusion_matrix
import time



#geting the image shape

for images, labels in train_dataset.take(1):
    input_shape = images.shape[1:]

#Loading VGG16 model

base_model = VGG16(weights="imagenet", include_top=False, input_shape=input_shape)
base_model.trainable = False ## freeze the top layers of the base model by set the trainable weights to false 

# Define a preprocessing function

def preprocess_image(image, label):
    image = preprocess_input(image)
    return image, label


# Apply the preprocessing function to the dataset

train_dataset = train_dataset.map(preprocess_image)
validation_dataset = validation_dataset.map(preprocess_image)
base_model.summary()

##

##adding the layers on top of the VGG base model

flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50, activation='relu')
dense_layer_2 = layers.Dense(40, activation='relu')
prediction_layer = layers.Dense(30, activation='softmax')


model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
])




#Compiling the model

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', Precision(), Recall()],
)


es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)

start_time = time.time()
model.fit(train_dataset, epochs=50, validation_data= validation_dataset, callbacks=[es])
end_time = time.time()

elapsed_time = end_time - start_time
print(f'Training time: {elapsed_time:.2f} seconds')

#Evaluation of the model

val_loss, val_accuracy, val_precision, val_recall = model.evaluate(validation_dataset)
print(f'Validation Loss: {val_loss}')
print(f'Validation Accuracy: {val_accuracy}')
print(f'Validation Precision: {val_precision}')
print(f'Validation Recall: {val_recall}')



# Fine-Tuning by unfreeze the top layers of the base model
base_model.trainable = True

# Recompile the model with a lower learning rate
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Lower learning rate
    loss='categorical_crossentropy',
    metrics=['accuracy', Precision(), Recall()],
)

# Continue training (fine-tuning) the model
start_time = time.time()
model.fit(train_dataset, epochs=10, validation_data=validation_dataset, callbacks=[es])
end_time = time.time()

elapsed_time = end_time - start_time
print(f'Training time: {elapsed_time:.2f} seconds')

# Evaluate the model again on the validation dataset
val_loss, val_accuracy, val_precision, val_recall = model.evaluate(validation_dataset)
print(f'Validation Loss (after fine-tuning): {val_loss}')
print(f'Validation Accuracy (after fine-tuning): {val_accuracy}')
print(f'Validation Precision (after fine-tuning): {val_precision}')
print(f'Validation Recall (after fine-tuning): {val_recall}')



#Confusion matrix

# Predict the class probabilities on the validation dataset
y_pred_probs = model.predict(validation_dataset)

# Convert probabilities to class labels
y_pred = np.argmax(y_pred_probs, axis=1)

y_true = []
for images, labels in validation_dataset:
    y_true.extend(np.argmax(labels.numpy(), axis=1))
y_true = np.array(y_true)



# Create confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()